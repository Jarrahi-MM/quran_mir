{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Based Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6236/6236 [00:00<00:00, 9123.42it/s]\n",
      "100%|██████████| 6236/6236 [00:00<00:00, 14811.03it/s]\n",
      "100%|██████████| 6236/6236 [00:00<00:00, 11727.70it/s]\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.10) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from preprocess_quran_text import merged_quran_vec_df_nrmlz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import qalsadi.lemmatizer\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, pipeline, \\\n",
    "    AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "import pandas as pd\n",
    "import hazm\n",
    "import torch\n",
    "\n",
    "\n",
    "stemmer = ISRIStemmer()\n",
    "lemmer = qalsadi.lemmatizer.Lemmatizer()  # This is a weak Lemmatizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_normalized</th>\n",
       "      <th>lemma_normalized</th>\n",
       "      <th>root_normalized</th>\n",
       "      <th>شماره سوره</th>\n",
       "      <th>شماره آیه</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2##1</th>\n",
       "      <td>الم</td>\n",
       "      <td>الٓمٓ</td>\n",
       "      <td>الٓمٓ</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2##2</th>\n",
       "      <td>ذلك الكتاب لا ريب فيه هدي للمتقين</td>\n",
       "      <td>ذلك كتب لا ريب في هدي متقين</td>\n",
       "      <td>كتب ريب هدي وقي</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2##3</th>\n",
       "      <td>الذين يومنون بالغيب و يقيمون الصلاه و مما رزقن...</td>\n",
       "      <td>الذي امن غيب اقام صلوه من ما رزق انفق</td>\n",
       "      <td>امن غيب قوم صلو رزق نفق</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2##4</th>\n",
       "      <td>و الذين يومنون بما انزل اليك و ما انزل من قبلك...</td>\n",
       "      <td>الذي امن ما انزل الي ما انزل من قبل آخر يوقن</td>\n",
       "      <td>امن نزل نزل قبل اخر يقن</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2##5</th>\n",
       "      <td>اولئك علي هدي من ربهم و اولئك هم المفلحون</td>\n",
       "      <td>اولٓئك علي هدي من رب اولٓئك مفلحون</td>\n",
       "      <td>هدي ربب فلح</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56##92</th>\n",
       "      <td>و اما ان كان من المكذبين الضالين</td>\n",
       "      <td>اما ان كان من مكذبين ضآل</td>\n",
       "      <td>كون كذب ضلل</td>\n",
       "      <td>56</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56##93</th>\n",
       "      <td>فنزل من حميم</td>\n",
       "      <td>نزل من حميم</td>\n",
       "      <td>نزل حمم</td>\n",
       "      <td>56</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56##94</th>\n",
       "      <td>و تصليه جحيم</td>\n",
       "      <td>تصليه جحيم</td>\n",
       "      <td>صلي جحم</td>\n",
       "      <td>56</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56##95</th>\n",
       "      <td>ان هذا لهو حق اليقين</td>\n",
       "      <td>ان هذا حق يقين</td>\n",
       "      <td>حقق يقن</td>\n",
       "      <td>56</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56##96</th>\n",
       "      <td>فسبح باسم ربك العظيم</td>\n",
       "      <td>سبح اسم رب عظيم</td>\n",
       "      <td>سبح سمو ربب عظم</td>\n",
       "      <td>56</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      original_normalized  \\\n",
       "index                                                       \n",
       "2##1                                                  الم   \n",
       "2##2                    ذلك الكتاب لا ريب فيه هدي للمتقين   \n",
       "2##3    الذين يومنون بالغيب و يقيمون الصلاه و مما رزقن...   \n",
       "2##4    و الذين يومنون بما انزل اليك و ما انزل من قبلك...   \n",
       "2##5            اولئك علي هدي من ربهم و اولئك هم المفلحون   \n",
       "...                                                   ...   \n",
       "56##92                   و اما ان كان من المكذبين الضالين   \n",
       "56##93                                       فنزل من حميم   \n",
       "56##94                                       و تصليه جحيم   \n",
       "56##95                               ان هذا لهو حق اليقين   \n",
       "56##96                               فسبح باسم ربك العظيم   \n",
       "\n",
       "                                     lemma_normalized  \\\n",
       "index                                                   \n",
       "2##1                                            الٓمٓ   \n",
       "2##2                      ذلك كتب لا ريب في هدي متقين   \n",
       "2##3            الذي امن غيب اقام صلوه من ما رزق انفق   \n",
       "2##4    الذي امن ما انزل الي ما انزل من قبل آخر يوقن   \n",
       "2##5               اولٓئك علي هدي من رب اولٓئك مفلحون   \n",
       "...                                               ...   \n",
       "56##92                      اما ان كان من مكذبين ضآل   \n",
       "56##93                                    نزل من حميم   \n",
       "56##94                                     تصليه جحيم   \n",
       "56##95                                 ان هذا حق يقين   \n",
       "56##96                                سبح اسم رب عظيم   \n",
       "\n",
       "                root_normalized  شماره سوره  شماره آیه  \n",
       "index                                                   \n",
       "2##1                      الٓمٓ           2          1  \n",
       "2##2            كتب ريب هدي وقي           2          2  \n",
       "2##3    امن غيب قوم صلو رزق نفق           2          3  \n",
       "2##4    امن نزل نزل قبل اخر يقن           2          4  \n",
       "2##5                هدي ربب فلح           2          5  \n",
       "...                         ...         ...        ...  \n",
       "56##92              كون كذب ضلل          56         92  \n",
       "56##93                  نزل حمم          56         93  \n",
       "56##94                  صلي جحم          56         94  \n",
       "56##95                  حقق يقن          56         95  \n",
       "56##96          سبح سمو ربب عظم          56         96  \n",
       "\n",
       "[3800 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = merged_quran_vec_df_nrmlz[['original_normalized', 'lemma_normalized', 'root_normalized']].copy()\n",
    "X['شماره سوره'] = X.index.to_series().str.split('##').apply(lambda x: int(x[0]))\n",
    "X['شماره آیه'] = X.index.to_series().str.split('##').apply(lambda x: int(x[1]))\n",
    "\n",
    "top_sures = X.groupby('شماره سوره').count().sort_values(by='شماره آیه', ascending=False).reset_index()['شماره سوره'][:30]\n",
    "top_x = X[X['شماره سوره'].isin(top_sures)]\n",
    "top_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.04it/s]\n",
      "3800it [01:15, 50.27it/s] \n",
      "3800it [00:50, 74.59it/s] \n",
      "3800it [00:49, 77.34it/s] \n",
      "3800it [00:50, 75.83it/s] \n",
      "3800it [00:49, 77.35it/s] \n"
     ]
    }
   ],
   "source": [
    "def create_data_set(out_dir: str, merged_df: pd.DataFrame, lemma_rate=0, root_rate=0, expansion_count=0):\n",
    "    with open(out_dir, 'w') as fp:\n",
    "        fp.write('text,labels\\n')\n",
    "        for col in tqdm.tqdm(['original_normalized', 'lemma_normalized', 'root_normalized']):\n",
    "            for i, r in merged_df.iterrows():\n",
    "                fp.write(r[col] + ',' + str(r['شماره سوره']))\n",
    "                fp.write('\\n')\n",
    "        probabilities = [1 - lemma_rate - root_rate, lemma_rate, root_rate]\n",
    "        functions = [lambda x: x, lemmer.lemmatize, stemmer.stem]\n",
    "        for _ in range(expansion_count):  # can be enhanced\n",
    "            for i, r in tqdm.tqdm(merged_df.iterrows()):\n",
    "                fp.write(\n",
    "                    ' '.join([np.random.choice(a=functions, p=probabilities)(token)\n",
    "                                         for token in r['original_normalized'].split()])\n",
    "                )\n",
    "                fp.write(',' + str(r['شماره سوره']))\n",
    "                fp.write('\\n')\n",
    "\n",
    "create_data_set(out_dir='transformer_classification_dataset.csv', merged_df=top_x,\n",
    "                                                                 lemma_rate=0.2,\n",
    "                                                                 root_rate=0.2,\n",
    "                                                                  expansion_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel:\n",
    "\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.id2label = None\n",
    "        self.label2id = None\n",
    "\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                                        num_labels=num_labels)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.classifier = pipeline(\n",
    "            \"sentiment-analysis\", model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def training_args_builder(output_dir=\"models/\", learning_rate=2e-3, train_batch_size=32,\n",
    "                              eval_batch_size=32, num_train_epochs=30, weight_decay=0.01):\n",
    "        return TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=train_batch_size,\n",
    "            per_device_eval_batch_size=eval_batch_size,\n",
    "            weight_decay=weight_decay,\n",
    "            learning_rate=learning_rate,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            save_steps=1_000\n",
    "        )\n",
    "\n",
    "    def get_train_valid_test(self, dataset, valid_ratio=0.1, test_ratio=0.1):\n",
    "        train_dataset, test_valid_dataset = train_test_split(\n",
    "            dataset, test_size=(valid_ratio + test_ratio), random_state=42, shuffle=True\n",
    "        )\n",
    "        valid_dataset, test_dataset = train_test_split(\n",
    "            test_valid_dataset, test_size=((test_ratio)/(valid_ratio + test_ratio)),\n",
    "            random_state=42, shuffle=True\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            Dataset.from_dict(train_dataset),\n",
    "            Dataset.from_dict(valid_dataset),\n",
    "            Dataset.from_dict(test_dataset)\n",
    "        )\n",
    "\n",
    "    def train(self, train_data, valid_data, training_args=None):\n",
    "        training_args = training_args or self.training_args_builder()\n",
    "\n",
    "        train_dataset, valid_dataset = \\\n",
    "            Dataset.from_dict(train_data), Dataset.from_dict(valid_data)\n",
    "\n",
    "        dataset = DatasetDict({\"train\": train_dataset, \"valid\": valid_dataset})\n",
    "\n",
    "        def preprocess_function(data):\n",
    "            return self.tokenizer(data[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "        tokenized_data = dataset.map(\n",
    "            preprocess_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "\n",
    "        optimizer = Adafactor(\n",
    "            self.model.parameters(), scale_parameter=True, \n",
    "                                    relative_step=True,\n",
    "                                    warmup_init=True, lr=None)\n",
    "        lr_scheduler = AdafactorSchedule(optimizer)\n",
    "\n",
    "        Trainer(\n",
    "            eval_dataset=tokenized_data[\"valid\"],\n",
    "            train_dataset=tokenized_data[\"train\"],\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            optimizers=(optimizer, lr_scheduler),\n",
    "        ).train()\n",
    "\n",
    "    def predict(self, test_data):\n",
    "        self.classifier.model.to('cpu')\n",
    "        inner_labels = self.classifier(texts)\n",
    "        return list(map(lambda lab: int(lab['label'].split('_')[1]), inner_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الم</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ذلك الكتاب لا ريب فيه هدي للمتقين</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الذين يومنون بالغيب و يقيمون الصلاه و مما رزقن...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>و الذين يومنون بما انزل اليك و ما انزل من قبلك...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اولئك علي هدي من ربهم و اولئك هم المفلحون</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0                                                الم       2\n",
       "1                  ذلك الكتاب لا ريب فيه هدي للمتقين       2\n",
       "2  الذين يومنون بالغيب و يقيمون الصلاه و مما رزقن...       2\n",
       "3  و الذين يومنون بما انزل اليك و ما انزل من قبلك...       2\n",
       "4          اولئك علي هدي من ربهم و اولئك هم المفلحون       2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./transformer_classification_dataset.csv', converters={'labels':int})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd3486fa5d44262b5e74041ce71fed3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'FloatProgress' object has no attribute 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/dana/Desktop/University/AIR/codes/collab_prj/quran_mir/transformer_classification.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dana/Desktop/University/AIR/codes/collab_prj/quran_mir/transformer_classification.ipynb#ch0000016?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m ClassificationModel(model_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39maubmindlab/bert-base-arabertv2\u001b[39;49m\u001b[39m'\u001b[39;49m, num_labels\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dana/Desktop/University/AIR/codes/collab_prj/quran_mir/transformer_classification.ipynb#ch0000016?line=1'>2</a>\u001b[0m a, b, c \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_train_valid_test(df)\n",
      "\u001b[1;32m/home/dana/Desktop/University/AIR/codes/collab_prj/quran_mir/transformer_classification.ipynb Cell 8\u001b[0m in \u001b[0;36mClassificationModel.__init__\u001b[0;34m(self, model_name, num_labels)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dana/Desktop/University/AIR/codes/collab_prj/quran_mir/transformer_classification.ipynb#ch0000016?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid2label \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dana/Desktop/University/AIR/codes/collab_prj/quran_mir/transformer_classification.ipynb#ch0000016?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel2id \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dana/Desktop/University/AIR/codes/collab_prj/quran_mir/transformer_classification.ipynb#ch0000016?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(model_name,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dana/Desktop/University/AIR/codes/collab_prj/quran_mir/transformer_classification.ipynb#ch0000016?line=8'>9</a>\u001b[0m                                                                 num_labels\u001b[39m=\u001b[39;49mnum_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dana/Desktop/University/AIR/codes/collab_prj/quran_mir/transformer_classification.ipynb#ch0000016?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dana/Desktop/University/AIR/codes/collab_prj/quran_mir/transformer_classification.ipynb#ch0000016?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dana/Desktop/University/AIR/codes/collab_prj/quran_mir/transformer_classification.ipynb#ch0000016?line=12'>13</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msentiment-analysis\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, tokenizer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:423\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39m_from_auto\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 423\u001b[0m     config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    424\u001b[0m         pretrained_model_name_or_path, return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    425\u001b[0m     )\n\u001b[1;32m    426\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n\u001b[1;32m    427\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:705\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    704\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 705\u001b[0m config_dict, _ \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    706\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    707\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:553\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    552\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    555\u001b[0m \u001b[39m# That config file may point us toward another config file to use.\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mconfiguration_files\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:601\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m         config_file \u001b[39m=\u001b[39m hf_bucket_url(\n\u001b[1;32m    596\u001b[0m             pretrained_model_name_or_path, filename\u001b[39m=\u001b[39mconfiguration_file, revision\u001b[39m=\u001b[39mrevision, mirror\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    597\u001b[0m         )\n\u001b[1;32m    599\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 601\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_path(\n\u001b[1;32m    602\u001b[0m         config_file,\n\u001b[1;32m    603\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    604\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    605\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    606\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    607\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    608\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    609\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    612\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    613\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    614\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier listed on \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    615\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a token having \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    616\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpermission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    617\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    618\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/hub.py:284\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    280\u001b[0m     local_files_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    283\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    285\u001b[0m         url_or_filename,\n\u001b[1;32m    286\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    287\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    288\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    289\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    290\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    291\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    292\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    293\u001b[0m     )\n\u001b[1;32m    294\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    295\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/hub.py:594\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m    592\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m not found in cache or force_download set to True, downloading to \u001b[39m\u001b[39m{\u001b[39;00mtemp_file\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 594\u001b[0m     http_get(url_to_download, temp_file, proxies\u001b[39m=\u001b[39;49mproxies, resume_size\u001b[39m=\u001b[39;49mresume_size, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    596\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mcache_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    597\u001b[0m os\u001b[39m.\u001b[39mreplace(temp_file\u001b[39m.\u001b[39mname, cache_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/hub.py:438\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m    435\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m# `tqdm` behavior is determined by `utils.logging.is_progress_bar_enabled()`\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m# and can be set using `utils.logging.enable/disable_progress_bar()`\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    439\u001b[0m     unit\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mB\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    440\u001b[0m     unit_scale\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    441\u001b[0m     unit_divisor\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m,\n\u001b[1;32m    442\u001b[0m     total\u001b[39m=\u001b[39;49mtotal,\n\u001b[1;32m    443\u001b[0m     initial\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m    444\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDownloading\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m )\n\u001b[1;32m    446\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m    447\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/logging.py:312\u001b[0m, in \u001b[0;36m_tqdm_cls.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m _tqdm_active:\n\u001b[0;32m--> 312\u001b[0m         \u001b[39mreturn\u001b[39;00m tqdm_lib\u001b[39m.\u001b[39;49mtqdm(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    313\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m         \u001b[39mreturn\u001b[39;00m EmptyTqdm(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplay\n\u001b[0;32m--> 249\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolour \u001b[39m=\u001b[39m colour\n\u001b[1;32m    251\u001b[0m \u001b[39m# Print initial bar state\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/notebook.py:208\u001b[0m, in \u001b[0;36mtqdm_notebook.colour\u001b[0;34m(self, bar_color)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39m@colour\u001b[39m\u001b[39m.\u001b[39msetter\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcolour\u001b[39m(\u001b[39mself\u001b[39m, bar_color):\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcontainer\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontainer\u001b[39m.\u001b[39;49mchildren[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39;49mstyle\u001b[39m.\u001b[39mbar_color \u001b[39m=\u001b[39m bar_color\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FloatProgress' object has no attribute 'style'"
     ]
    }
   ],
   "source": [
    "model = ClassificationModel(model_name='aubmindlab/bert-base-arabertv2', num_labels=30)\n",
    "a, b, c = model.get_train_valid_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
